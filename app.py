import gradio as gr
import os
from dotenv import load_dotenv
import glob
import math
import json
import pandas as pd
import tempfile

# .env íŒŒì¼ì„ ìµœìƒë‹¨ì—ì„œ ë¡œë“œ
load_dotenv()

# --- ì‹œì‘ ì‹œ ì„ì‹œ ì´ë¯¸ì§€ í´ë” ì •ë¦¬ ---
image_temp_dir = "image_temp"
if os.path.exists(image_temp_dir):
    files_to_delete = glob.glob(os.path.join(image_temp_dir, '*.png'))
    for f in files_to_delete:
        try:
            os.remove(f)
        except OSError as e:
            print(f"Error removing file {f}: {e}")

# --- ëª¨ë“ˆì—ì„œ ê¸°ëŠ¥ë“¤ì„ ê°€ì ¸ì˜´ ---
from modules.location_search.location import get_location_js
from modules.location_search.search import find_nearby_places
from modules.area_search.controls import (
    AREA_CODES, CONTENT_TYPE_CODES, update_sigungu_dropdown
)
from modules.area_search.search import update_page_view
from modules.area_search.details import get_details
from modules.area_search.export import export_to_csv
from modules.trend_analyzer.trend_analyzer import (
    generate_trends_from_area_search,
    generate_trends_from_location_search,
    analyze_single_item,
    analyze_trends_for_titles
)
# ì„œìš¸ ê´€ê´‘ API ëª¨ë“ˆ
from modules.seoul_search.seoul_api import get_all_seoul_data
# ë„¤ì´ë²„ ê²€ìƒ‰ ëª¨ë“ˆ
from modules.naver_search.search import search_naver_reviews_and_scrape, summarize_blog_contents_stream, answer_question_from_reviews_stream
# Tour API ì§ì ‘ ì¡°íšŒ ëª¨ë“ˆ
from modules.tour_api_playwright_search import scraper
import asyncio
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
import requests
import re


# --- ì„œìš¸ì‹œ ê´€ê´‘ ì •ë³´ ê²€ìƒ‰ UI ë° ê¸°ëŠ¥ ---

ROWS_PER_PAGE = 10
PAGE_WINDOW_SIZE = 5

# ì¹´í…Œê³ ë¦¬ ì´ë¦„ê³¼ íƒœê·¸ í‚¤ì›Œë“œ ë§¤í•‘
CATEGORY_TO_KEYWORDS = {
    "ê´€ê´‘ì§€": ["ê´€ê´‘", "ëª…ì†Œ", "ìœ ì "],
    "ë¬¸í™”ì‹œì„¤": ["ë¬¸í™”", "ë¯¸ìˆ ê´€", "ë°•ë¬¼ê´€", "ì „ì‹œ", "ê°¤ëŸ¬ë¦¬", "ë„ì„œê´€"],
    "í–‰ì‚¬/ê³µì—°/ì¶•ì œ": ["í–‰ì‚¬", "ê³µì—°", "ì¶•ì œ", "í˜ìŠ¤í‹°ë²Œ"],
    "ì—¬í–‰ì½”ìŠ¤": ["ì—¬í–‰ì½”ìŠ¤", "ë„ë³´", "ì‚°ì±…", "ë‘˜ë ˆê¸¸"],
    "ë ˆí¬ì¸ ": ["ë ˆí¬ì¸ ", "ìŠ¤í¬ì¸ ", "ê³µì›", "ì²´ìœ¡"],
    "ìˆ™ë°•": ["ìˆ™ë°•", "í˜¸í…”", "ëª¨í…”", "ê²ŒìŠ¤íŠ¸í•˜ìš°ìŠ¤", "íœì…˜"],
    "ì‡¼í•‘": ["ì‡¼í•‘", "ë°±í™”ì ", "ì‹œì¥", "ë©´ì„¸ì "],
    "ìŒì‹ì ": ["ìŒì‹ì ", "ë§›ì§‘", "ì‹ë‹¹", "ì¹´í˜"],
}

def create_seoul_search_ui():
    """ì„œìš¸ì‹œ ê´€ê´‘ì •ë³´ APIìš© UI íƒ­ (ëª¨ë“  ê¸°ëŠ¥ í¬í•¨)"""
    with gr.Blocks() as seoul_search_tab:
        # --- ìƒíƒœ ë³€ìˆ˜ ---
        filtered_data_state = gr.State([])
        current_page_state = gr.State(1)
        total_pages_state = gr.State(1)

        # --- UI ì»´í¬ë„ŒíŠ¸ ---
        gr.Markdown("### ì„œìš¸ì‹œ ê´€ê´‘ì§€ ê²€ìƒ‰ (ì¹´í…Œê³ ë¦¬ë³„ í•„í„°ë§)")
        with gr.Row():
            category_dropdown = gr.Dropdown(label="ì¹´í…Œê³ ë¦¬", choices=list(CONTENT_TYPE_CODES.keys()), value="ì „ì²´")
            search_btn = gr.Button("ê²€ìƒ‰í•˜ê¸°", variant="primary")
        
        with gr.Row():
            export_csv_btn = gr.Button("CSVë¡œ ë‚´ë³´ë‚´ê¸°")
            run_list_trend_btn = gr.Button("í˜„ì¬ ëª©ë¡ íŠ¸ë Œë“œ ì €ì¥í•˜ê¸°")

        places_radio = gr.Radio(label="ê²€ìƒ‰ëœ ê´€ê´‘ì§€ ëª©ë¡", choices=[], interactive=True)
        
        with gr.Row(visible=False) as pagination_row:
            first_page_btn = gr.Button("ë§¨ ì²˜ìŒ", interactive=False)
            prev_page_btn = gr.Button("ì´ì „", interactive=False)
            pagination_numbers = gr.Radio(choices=[], label="í˜ì´ì§€", interactive=True, scale=2)
            next_page_btn = gr.Button("ë‹¤ìŒ", interactive=False)
            last_page_btn = gr.Button("ë§¨ ë", interactive=False)
        
        csv_file_output = gr.File(label="CSV ë‹¤ìš´ë¡œë“œ", interactive=False)
        status_output = gr.Textbox(label="ë¶„ì„ ìƒíƒœ", interactive=False, lines=2)

        with gr.Accordion("ìƒì„¸ ì •ë³´ ë° ë¶„ì„ ê²°ê³¼", open=False) as details_accordion:
            raw_json_output = gr.Textbox(label="ìƒì„¸ ì •ë³´ (Raw JSON)", lines=10, interactive=False)
            pretty_output = gr.Markdown("### í¬ë§·ëœ ì •ë³´")
            trend_plot_output = gr.Image(label="ê²€ìƒ‰ëŸ‰ íŠ¸ë Œë“œ", interactive=False)
            reviews_output = gr.Markdown("### ë„¤ì´ë²„ ë¸”ë¡œê·¸ í›„ê¸°")

        # --- ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
        search_btn.click(
            fn=perform_search,
            inputs=[category_dropdown],
            outputs=[filtered_data_state, current_page_state, status_output, csv_file_output]
        ).then(
            fn=update_seoul_page_view,
            inputs=[filtered_data_state, current_page_state],
            outputs=[
                places_radio, pagination_row, 
                first_page_btn, prev_page_btn, next_page_btn, last_page_btn, 
                pagination_numbers, total_pages_state
            ]
        )

        export_csv_btn.click(
            fn=export_seoul_data_to_csv,
            inputs=[filtered_data_state],
            outputs=[csv_file_output]
        )

        run_list_trend_btn.click(
            fn=run_seoul_list_trend_analysis,
            inputs=[filtered_data_state],
            outputs=[status_output]
        )

        page_change_triggers = [
            first_page_btn.click(lambda: 1, [], current_page_state),
            prev_page_btn.click(lambda p: p - 1, [current_page_state], current_page_state),
            next_page_btn.click(lambda p: p + 1, [current_page_state], current_page_state),
            last_page_btn.click(lambda tp: tp, [total_pages_state], current_page_state),
            pagination_numbers.select(lambda evt: int(evt.value), pagination_numbers, current_page_state)
        ]

        for trigger in page_change_triggers:
            trigger.then(
                fn=update_seoul_page_view,
                inputs=[filtered_data_state, current_page_state],
                outputs=[
                    places_radio, pagination_row, 
                    first_page_btn, prev_page_btn, next_page_btn, last_page_btn, 
                    pagination_numbers, total_pages_state
                ]
            )
        
        places_radio.change(
            fn=display_details_and_analysis,
            inputs=[places_radio, filtered_data_state],
            outputs=[raw_json_output, pretty_output, trend_plot_output, reviews_output, details_accordion]
        )

    return seoul_search_tab

def perform_search(category_name):
    all_data = get_all_seoul_data()
    if not all_data:
        gr.Warning("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return [], 1, "", None

    if category_name == "ì „ì²´":
        filtered_list = all_data
    else:
        keywords = CATEGORY_TO_KEYWORDS.get(category_name, [])
        filtered_list = [item for item in all_data if item['processed'].get('tags') and any(keyword in item['processed']['tags'] for keyword in keywords)]
    
    if not filtered_list:
        gr.Info(f"'{category_name}' ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    return filtered_list, 1, "", None

def update_seoul_page_view(filtered_data, page_to_go):
    if not filtered_data:
        return gr.update(choices=[], value=None), gr.update(visible=False), False, False, False, False, gr.update(choices=[], value=None), 1

    page_to_go = int(page_to_go)
    total_count = len(filtered_data)
    total_pages = math.ceil(total_count / ROWS_PER_PAGE)

    start_idx = (page_to_go - 1) * ROWS_PER_PAGE
    end_idx = start_idx + ROWS_PER_PAGE
    page_items = filtered_data[start_idx:end_idx]

    place_titles = [item['processed']['title'] for item in page_items if item.get('processed', {}).get('title')]

    half_window = PAGE_WINDOW_SIZE // 2
    start_page = max(1, page_to_go - half_window)
    end_page = min(total_pages, start_page + PAGE_WINDOW_SIZE - 1)
    if end_page - start_page + 1 < PAGE_WINDOW_SIZE:
        start_page = max(1, end_page - PAGE_WINDOW_SIZE + 1)
    page_numbers_to_show = [str(i) for i in range(start_page, end_page + 1)]

    return (
        gr.update(choices=place_titles, value=None),
        gr.update(visible=total_pages > 1),
        gr.update(interactive=page_to_go > 1),
        gr.update(interactive=page_to_go < total_pages),
        gr.update(interactive=page_to_go < total_pages),
        gr.update(interactive=page_to_go < total_pages),
        gr.update(choices=page_numbers_to_show, value=str(page_to_go)),
        total_pages
    )

def display_details_and_analysis(selected_title, filtered_data, progress=gr.Progress(track_tqdm=True)):
    if not selected_title:
        return "", "", None, "", gr.update(open=False)

    progress(0, desc="ìƒì„¸ ì •ë³´ ë¡œë”© ì¤‘...")
    selected_item = next((item for item in filtered_data if item.get('processed', {}).get('title') == selected_title), None)

    if not selected_item:
        return "{}", "ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", None, "", gr.update(open=True)

    raw_data = selected_item.get('raw', {})
    raw_json_str = json.dumps(raw_data, indent=2, ensure_ascii=False)
    
    KEY_MAP = {
        "POST_SJ": "ìƒí˜¸ëª…", "NEW_ADDRESS": "ìƒˆì£¼ì†Œ", "ADDRESS": "êµ¬ì£¼ì†Œ",
        "CMMN_TELNO": "ì „í™”ë²ˆí˜¸", "CMMN_HMPG_URL": "í™ˆí˜ì´ì§€", "CMMN_USE_TIME": "ì´ìš©ì‹œê°„",
        "CMMN_BSNDE": "ìš´ì˜ìš”ì¼", "CMMN_RSTDE": "íœ´ë¬´ì¼", "SUBWAY_INFO": "ì§€í•˜ì²  ì •ë³´",
        "TAG": "íƒœê·¸", "BF_DESC": "ì¥ì• ì¸ í¸ì˜ì‹œì„¤"
    }
    
    pretty_str_lines = [f"### {raw_data.get('POST_SJ', 'ì´ë¦„ ì—†ìŒ')}"]
    for key, friendly_name in KEY_MAP.items():
        value = raw_data.get(key)
        if value and str(value).strip():
            cleaned_value = str(value).replace('\n', ' ').strip()
            if key == 'CMMN_HMPG_URL' and 'http' in cleaned_value:
                pretty_str_lines.append(f"**{friendly_name}**: [{cleaned_value}]({cleaned_value})")
            else:
                pretty_str_lines.append(f"**{friendly_name}**: {cleaned_value}")
    pretty_str = "\n\n".join(pretty_str_lines)

    progress(0.5, desc="íŠ¸ë Œë“œ ë° í›„ê¸° ë¶„ì„ ì¤‘...")
    trend_image, reviews_markdown = analyze_single_item(selected_title)
    
    progress(1, desc="ì™„ë£Œ")
    return raw_json_str, pretty_str, trend_image, reviews_markdown, gr.update(open=True)

def export_seoul_data_to_csv(filtered_data, progress=gr.Progress(track_tqdm=True)):
    """í˜„ì¬ í•„í„°ë§ëœ ì„œìš¸ì‹œ ë°ì´í„°ë¥¼ CSV íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
    if not filtered_data:
        gr.Warning("ë‚´ë³´ë‚¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    progress(0, desc="CSV ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    raw_data_list = [item['raw'] for item in filtered_data]
    df = pd.DataFrame(raw_data_list)

    progress(0.5, desc="CSV íŒŒì¼ ìƒì„± ì¤‘...")
    with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.csv', prefix='seoul_attractions_', encoding='utf-8-sig') as temp_f:
        df.to_csv(temp_f.name, index=False, encoding='utf-8-sig') # ì¸ì½”ë”© ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€
        gr.Info("CSV íŒŒì¼ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        progress(1, desc="ì™„ë£Œ")
        return temp_f.name

def run_seoul_list_trend_analysis(filtered_data, progress=gr.Progress(track_tqdm=True)):
    """í˜„ì¬ í•„í„°ë§ëœ ëª©ë¡ ì „ì²´ì— ëŒ€í•œ íŠ¸ë Œë“œ/í›„ê¸° ë¶„ì„ì„ ì‹¤í–‰í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not filtered_data:
        return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    titles = [item['processed']['title'] for item in filtered_data if item.get('processed', {}).get('title')]
    if not titles:
        return "ë¶„ì„í•  ê´€ê´‘ì§€ ì´ë¦„ì´ ì—†ìŠµë‹ˆë‹¤."
        
    status = analyze_trends_for_titles(titles=titles, progress=progress)
    return status


# --- ê° íƒ­ì˜ UIë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ë“¤ ---

def create_location_search_tab():
    """'ë‚´ ìœ„ì¹˜ë¡œ ê²€ìƒ‰' íƒ­ì˜ UIë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    with gr.Blocks() as tab:
        gr.Markdown("### ë‚´ ìœ„ì¹˜ ê¸°ë°˜ ê´€ê´‘ì§€ ê²€ìƒ‰")
        places_info_state_nearby = gr.State({})
        with gr.Row():
            get_loc_button = gr.Button("ë‚´ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°")
            lat_box, lon_box = gr.Textbox(label="ìœ„ë„", interactive=False), gr.Textbox(label="ê²½ë„", interactive=False)
        
        with gr.Row():
            search_button_nearby = gr.Button("ì´ ì¢Œí‘œë¡œ ì£¼ë³€ ê´€ê´‘ì§€ ê²€ìƒ‰", variant="primary")
            run_trend_btn_nearby = gr.Button("í˜„ì¬ ëª©ë¡ íŠ¸ë Œë“œ ì €ì¥í•˜ê¸°")

        radio_list_nearby = gr.Radio(label="ê´€ê´‘ì§€ ëª©ë¡", interactive=True)
        status_output_nearby = gr.Textbox(label="ì‘ì—… ìƒíƒœ", interactive=False)
        
        with gr.Accordion("ìƒì„¸ ì •ë³´ ë³´ê¸°", open=False):
            common_raw_n, common_pretty_n = gr.Textbox(label="Raw JSON"), gr.Markdown()
            intro_raw_n, intro_pretty_n = gr.Textbox(label="Raw JSON"), gr.Markdown()
            info_raw_n, info_pretty_n = gr.Textbox(label="Raw JSON"), gr.Markdown()
        
        get_loc_button.click(fn=None, js=get_location_js, outputs=[lat_box, lon_box])
        search_button_nearby.click(fn=find_nearby_places, inputs=[lat_box, lon_box], outputs=[radio_list_nearby, places_info_state_nearby])
        run_trend_btn_nearby.click(fn=generate_trends_from_location_search, inputs=places_info_state_nearby, outputs=status_output_nearby)
        radio_list_nearby.change(fn=get_details, inputs=[radio_list_nearby, places_info_state_nearby], outputs=[common_raw_n, common_pretty_n, intro_raw_n, intro_pretty_n, info_raw_n, info_pretty_n])
    return tab

def create_area_search_tab():
    """'ì§€ì—­/ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰' íƒ­ì˜ UIë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    with gr.Blocks() as tab:
        gr.Markdown("### ì§€ì—­/ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ê´€ê´‘ì§€ ê²€ìƒ‰ (TourAPI)")
        current_area = gr.State(None)
        current_sigungu = gr.State(None)
        current_category = gr.State(None)
        current_page = gr.State(1)
        total_pages = gr.State(1)
        places_info_state_area = gr.State({})

        with gr.Row():
            area_dropdown = gr.Dropdown(label="ì§€ì—­", choices=list(AREA_CODES.keys()))
            sigungu_dropdown = gr.Dropdown(label="ì‹œêµ°êµ¬", interactive=False)
            category_dropdown = gr.Dropdown(label="ì¹´í…Œê³ ë¦¬", choices=list(CONTENT_TYPE_CODES.keys()), value="ì „ì²´")
        
        with gr.Row():
            search_by_area_btn = gr.Button("ê²€ìƒ‰í•˜ê¸°", variant="primary")
            export_csv_btn = gr.Button("CSVë¡œ ë‚´ë³´ë‚´ê¸°")
            run_trend_btn_area = gr.Button("í˜„ì¬ ëª©ë¡ íŠ¸ë Œë“œ ì €ì¥í•˜ê¸°")

        radio_list_area = gr.Radio(label="ê´€ê´‘ì§€ ëª©ë¡", interactive=True)
        
        with gr.Row(visible=False) as pagination_row:
            first_page_btn = gr.Button("<< ë§¨ ì²˜ìŒ")
            prev_page_btn = gr.Button("< ì´ì „")
            page_numbers_radio = gr.Radio(label="í˜ì´ì§€", interactive=True, scale=3)
            next_page_btn = gr.Button("ë‹¤ìŒ >")
            last_page_btn = gr.Button("ë§¨ ë >>")
        
        csv_file_output = gr.File(label="ë‹¤ìš´ë¡œë“œ", interactive=False)
        status_output_area = gr.Textbox(label="ì‘ì—… ìƒíƒœ", interactive=False)

        with gr.Accordion("ìƒì„¸ ì •ë³´ ë³´ê¸°", open=False):
            common_raw_a, common_pretty_a = gr.Textbox(label="Raw JSON"), gr.Markdown()
            intro_raw_a, intro_pretty_a = gr.Textbox(label="Raw JSON"), gr.Markdown()
            info_raw_a, info_pretty_a = gr.Textbox(label="Raw JSON"), gr.Markdown()
        
        outputs_for_page_change = [current_area, current_sigungu, current_category, current_page, total_pages, places_info_state_area, radio_list_area, page_numbers_radio, first_page_btn, prev_page_btn, next_page_btn, last_page_btn, pagination_row]
        
        area_dropdown.change(fn=update_sigungu_dropdown, inputs=area_dropdown, outputs=sigungu_dropdown)
        search_by_area_btn.click(fn=update_page_view, inputs=[area_dropdown, sigungu_dropdown, category_dropdown, gr.Number(value=1, visible=False)], outputs=outputs_for_page_change)
        
        export_csv_btn.click(fn=export_to_csv, inputs=[area_dropdown, sigungu_dropdown, category_dropdown], outputs=csv_file_output)
        run_trend_btn_area.click(fn=generate_trends_from_area_search, inputs=[area_dropdown, sigungu_dropdown, category_dropdown], outputs=status_output_area)

        page_inputs = [current_area, current_sigungu, current_category]
        first_page_btn.click(lambda area, sigungu, cat: update_page_view(area, sigungu, cat, 1), inputs=page_inputs, outputs=outputs_for_page_change)
        prev_page_btn.click(lambda area, sigungu, cat, page: update_page_view(area, sigungu, cat, page - 1), inputs=page_inputs + [current_page], outputs=outputs_for_page_change)
        next_page_btn.click(lambda area, sigungu, cat, page: update_page_view(area, sigungu, cat, page + 1), inputs=page_inputs + [current_page], outputs=outputs_for_page_change)
        last_page_btn.click(lambda area, sigungu, cat, pages: update_page_view(area, sigungu, cat, pages), inputs=page_inputs + [total_pages], outputs=outputs_for_page_change)
        page_numbers_radio.select(update_page_view, inputs=page_inputs + [page_numbers_radio], outputs=outputs_for_page_change)

        radio_list_area.change(fn=get_details, inputs=[radio_list_area, places_info_state_area], outputs=[common_raw_a, common_pretty_a, intro_raw_a, intro_pretty_a, info_raw_a, info_pretty_a])
    return tab



def parse_xml_to_dict(xml_string: str) -> dict:
    """Parses an XML string from the API into a flat dictionary."""
    if not xml_string or "<error>" in xml_string or not xml_string.strip().startswith('<?xml'):
        return {}
    try:
        root = ET.fromstring(xml_string)
        item_element = root.find('.//body/items/item')
        if item_element is None:
            return {}
        
        details = {}
        for child in item_element:
            if child.text and child.text.strip():
                clean_text = re.sub(r'<.*?>', '', child.text)
                details[child.tag] = clean_text.strip()
        return details
    except (ET.ParseError, TypeError):
        return {}

def parse_xml_to_ordered_list(xml_string: str) -> list[tuple[str, str]]:
    """Parses an XML string from the API into an ordered list of (key, value) tuples."""
    if not xml_string or "<error>" in xml_string or not xml_string.strip().startswith('<?xml'):
        return []
    try:
        root = ET.fromstring(xml_string)
        item_element = root.find('.//body/items/item')
        if item_element is None:
            return []
        
        details = []
        for child in item_element:
            if child.text and child.text.strip():
                clean_text = re.sub(r'<.*?>', '', child.text)
                details.append((child.tag, clean_text.strip()))
        return details
    except (ET.ParseError, TypeError):
        return []

async def export_details_to_csv(search_params, progress=gr.Progress(track_tqdm=True)):
    """Fetches ALL items, gets full details including initial search XML, and saves to a CSV file with specific column order."""
    ITEMS_PER_PAGE = 12
    TEMP_DIR = os.path.join(os.path.dirname(__file__), "Temp")
    os.makedirs(TEMP_DIR, exist_ok=True)

    progress(0, desc="Getting total number of items...")
    try:
        initial_params = search_params.copy()
        if initial_params.get("sigungu") == "ì „ì²´": initial_params["sigungu"] = None
        if initial_params.get("cat1") == "ì„ íƒ ì•ˆí•¨": initial_params["cat1"] = None
        if initial_params.get("cat2") == "ì„ íƒ ì•ˆí•¨": initial_params["cat2"] = None
        if initial_params.get("cat3") == "ì„ íƒ ì•ˆí•¨": initial_params["cat3"] = None

        _, _, _, total_count = await scraper.get_search_results(**initial_params, pageNo=1, temp_dir=TEMP_DIR)
        if total_count == 0:
            gr.Info("No items to export.")
            return None
        total_pages = math.ceil(total_count / ITEMS_PER_PAGE)
    except Exception as e:
        gr.Error(f"Failed to get total item count. Error: {e}")
        return None

    all_items = []
    progress(0.1, desc="Fetching item lists from all pages...")
    for page_num in progress.tqdm(range(1, total_pages + 1), desc=f"Fetching {total_pages} pages"):
        try:
            results, _, _, _ = await scraper.get_search_results(**initial_params, pageNo=page_num, temp_dir=TEMP_DIR, totalPages=total_pages)
            all_items.extend(results)
        except Exception as e:
            print(f"Warning: Failed to fetch page {page_num}. Error: {e}")
            continue
    
    if not all_items:
        gr.Info("Could not fetch any item lists.")
        return None

    all_attraction_details = []
    simple_search_keys, common_info_keys, intro_info_keys, repeat_info_keys = [], [], [], []
    seen_keys = set()

    def add_key(key, key_list):
        if key not in seen_keys:
            seen_keys.add(key)
            key_list.append(key)

    tabs_to_fetch = [
        ("ê³µí†µì •ë³´", common_info_keys), 
        ("ì†Œê°œì •ë³´", intro_info_keys), 
        ("ë°˜ë³µì •ë³´", repeat_info_keys)
    ]

    for item in progress.tqdm(all_items, desc="Collecting and ordering details for each attraction"):
        content_id = item.get("contentid")
        if not content_id: continue

        combined_details = {}

        initial_xml = item.get("initial_item_xml")
        if initial_xml:
            # The initial XML is a string representation of a single <item> tag
            # To parse it, we need to wrap it in a dummy root structure.
            try:
                root = ET.fromstring(f'<root>{initial_xml}</root>')
                simple_item_element = root.find('item')
                if simple_item_element is not None:
                    for child in simple_item_element:
                        if child.text and child.text.strip():
                            add_key(child.tag, simple_search_keys)
                            combined_details[child.tag] = child.text.strip()
            except ET.ParseError:
                print(f"Could not parse initial_item_xml for {content_id}")

        detail_params = initial_params.copy()
        detail_params['contentid'] = content_id

        for tab_name, key_list in tabs_to_fetch:
            detail_params["tab_name"] = tab_name
            try:
                xml_string = await scraper.get_item_detail_xml(detail_params)
                item_details_list = parse_xml_to_ordered_list(xml_string)
                for key, value in item_details_list:
                    add_key(key, key_list)
                    combined_details[key] = value
            except Exception as e:
                print(f"Error fetching tab '{tab_name}' for contentid '{content_id}': {e}")
                continue
        
        all_attraction_details.append(combined_details)

    if not all_attraction_details:
        gr.Info("No details could be collected.")
        return None

    progress(0.9, desc="Creating CSV file with specified order...")
    
    final_ordered_columns = simple_search_keys + common_info_keys + intro_info_keys + repeat_info_keys
    
    df = pd.DataFrame(all_attraction_details)
    
    existing_ordered_columns = [col for col in final_ordered_columns if col in df.columns]
    df = df[existing_ordered_columns]

    if 'homepage' in df.columns:
        df['homepage'] = df['homepage'].apply(lambda x: re.search(r'href=[""](.*?)[""]', str(x)).group(1) if re.search(r'href=[""](.*?)[""]', str(x)) else x)

    try:
        with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.csv', prefix='tour_details_all_', encoding='utf-8-sig') as temp_f:
            df.to_csv(temp_f.name, index=False)
            gr.Info("CSV file with all items has been created successfully.")
            return temp_f.name
    except Exception as e:
        gr.Error(f"Error saving CSV file: {e}")
        return None


def create_tour_api_playwright_tab():
    """'Tour API ì§ì ‘ ì¡°íšŒ (Playwright)' íƒ­ì˜ UIë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    # --- Constants ---
    ITEMS_PER_PAGE = 12
    TEMP_DIR = os.path.join(os.path.dirname(__file__), "Temp")
    os.makedirs(TEMP_DIR, exist_ok=True)
    NO_IMAGE_PLACEHOLDER_PATH = os.path.join(TEMP_DIR, "no_image.svg")
    if not os.path.exists(NO_IMAGE_PLACEHOLDER_PATH):
        svg_content = '''<svg width="100" height="100" xmlns="http://www.w3.org/2000/svg">
          <rect width="100%" height="100%" fill="#cccccc"/>
          <text x="50%" y="50%" font-family="Arial" font-size="12" fill="#333333" text-anchor="middle" alignment-baseline="middle">No Image</text>
        </svg>'''
        with open(NO_IMAGE_PLACEHOLDER_PATH, "w", encoding="utf-8") as f:
            f.write(svg_content)

    with gr.Blocks(css="""    
        #pagination {justify-content: center; align-items: center;} 
        #pagination .gr-box {max-width: 150px;} 
        #pagination .gr-box.label {max-width: 50px;}
        .tab-content-markdown table {border-collapse: collapse; width: 100%;}
        .tab-content-markdown tr {border-bottom: 1px solid #eee;}
        .tab-content-markdown td {padding: 8px;}
        .tab-content-markdown td:first-child {width: 30%; font-weight: bold; vertical-align: top;}
    """) as demo:
        # --- UI Components & State ---
        search_params = gr.State({})
        current_page = gr.State(1)
        total_pages = gr.State(1)
        current_gallery_data = gr.State([])
        selected_item_info = gr.State({})

        DEFAULT_LARGE_CATEGORIES = ["ì„ íƒ ì•ˆí•¨", "ìì—°", "ì¸ë¬¸(ë¬¸í™”/ì˜ˆìˆ /ì—­ì‚¬)", "ë ˆí¬ì¸ ", "ì‡¼í•‘", "ìŒì‹", "ìˆ™ë°•", "ì¶”ì²œì½”ìŠ¤"]

        gr.Markdown("# TourAPI 4.0 ì²´í—˜ (Playwright + Gradio)")
        gr.Markdown("í•„í„°ë¥¼ ì„ íƒí•˜ê³  ê´€ê´‘ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")

        with gr.Row():
            with gr.Column(scale=1):
                language_dropdown = gr.Dropdown(label="ì–¸ì–´", choices=list(scraper.LANGUAGE_MAP.keys()), value="í•œêµ­ì–´", interactive=True)
                province_dropdown = gr.Dropdown(label="ê´‘ì—­ì‹œ/ë„", choices=["ì „êµ­", "ì„œìš¸", "ì¸ì²œ", "ëŒ€ì „", "ëŒ€êµ¬", "ê´‘ì£¼", "ë¶€ì‚°", "ìš¸ì‚°", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ", "ê²½ê¸°ë„", "ê°•ì›íŠ¹ë³„ìì¹˜ë„", "ì¶©ì²­ë¶ë„", "ì¶©ì²­ë‚¨ë„", "ê²½ìƒë¶ë„", "ê²½ìƒë‚¨ë„", "ì „ë¶íŠ¹ë³„ìì¹˜ë„", "ì „ë¼ë‚¨ë„", "ì œì£¼íŠ¹ë³„ìì¹˜ë„"], value="ì „êµ­", interactive=True)
                sigungu_dropdown = gr.Dropdown(label="ì‹œ/êµ°/êµ¬", choices=[], interactive=True)
                tourism_type_dropdown = gr.Dropdown(label="ê´€ê´‘íƒ€ì…", choices=["ì„ íƒ ì•ˆí•¨", "ê´€ê´‘ì§€", "ë¬¸í™”ì‹œì„¤", "ì¶•ì œê³µì—°í–‰ì‚¬", "ì—¬í–‰ì½”ìŠ¤", "ë ˆí¬ì¸ ", "ìˆ™ë°•", "ì‡¼í•‘", "ìŒì‹ì "], value="ì„ íƒ ì•ˆí•¨", interactive=True)
                large_category_dropdown = gr.Dropdown(label="ì„œë¹„ìŠ¤ ë¶„ë¥˜ (ëŒ€ë¶„ë¥˜)", choices=DEFAULT_LARGE_CATEGORIES, value="ì„ íƒ ì•ˆí•¨", interactive=True)
                medium_category_dropdown = gr.Dropdown(label="ì„œë¹„ìŠ¤ ë¶„ë¥˜ (ì¤‘ë¶„ë¥˜)", choices=[], interactive=True)
                small_category_dropdown = gr.Dropdown(label="ì„œë¹„ìŠ¤ ë¶„ë¥˜ (ì†Œë¶„ë¥˜)", choices=[], interactive=True)
                search_button = gr.Button("ê²€ìƒ‰", variant="primary")
                export_csv_button = gr.Button("ê²°ê³¼ ì „ì²´ CSV ì €ì¥")
            
            with gr.Column(scale=3):
                status_output = gr.Textbox(label="ìƒíƒœ", interactive=False)
                csv_output_file = gr.File(label="CSV ë‹¤ìš´ë¡œë“œ", interactive=False)
                results_output = gr.Gallery(label="ê²€ìƒ‰ ê²°ê³¼", show_label=False, elem_id="gallery", columns=4, height="auto", object_fit="contain", preview=True)
                
                with gr.Row(elem_id="pagination", variant="panel"):
                    first_page_button = gr.Button("<< ë§¨ ì²˜ìŒ")
                    prev_page_button = gr.Button("< ì´ì „")
                    page_number_input = gr.Number(label="", value=1, interactive=True, precision=0, minimum=1)
                    total_pages_output = gr.Textbox(label="/", value="/ 1", interactive=False, max_lines=1)
                    next_page_button = gr.Button("ë‹¤ìŒ >")
                    last_page_button = gr.Button("ë§¨ ë >>")

                with gr.Accordion("API ìš”ì²­/ì‘ë‹µ", visible=False) as api_accordion:
                    request_url_output = gr.Textbox(label="ìš”ì²­ URL", interactive=False)
                    response_xml_output = gr.Code(label="ì‘ë‹µ XML", interactive=False)

                with gr.Column(visible=False, elem_id="detail_view") as detail_view_column:
                    detail_title = gr.Markdown("### ì œëª©")
                    with gr.Tabs(elem_id="detail_tabs") as detail_tabs:
                        with gr.TabItem("ê³µí†µì •ë³´", id="ê³µí†µì •ë³´"):
                            detail_image = gr.Image(label="ëŒ€í‘œ ì´ë¯¸ì§€", interactive=False, height=300)
                            detail_info_table = gr.Markdown(elem_id="detail-info-table", elem_classes="tab-content-markdown")
                            detail_overview = gr.Textbox(label="ê°œìš”", interactive=False, lines=6)
                            show_map_button = gr.Button("ì§€ë„ë³´ê¸°", variant="secondary")
                            with gr.Group(visible=False) as map_group:
                                map_html = gr.HTML(elem_id="map-iframe", label="ì§€ë„")
                                close_map_button = gr.Button("ì§€ë„ ë‹«ê¸°")
                        with gr.TabItem("ì†Œê°œì •ë³´", id="ì†Œê°œì •ë³´"):
                            intro_info_markdown = gr.Markdown("ì†Œê°œì •ë³´ íƒ­ì„ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.", elem_classes="tab-content-markdown")
                        with gr.TabItem("ë°˜ë³µì •ë³´", id="ë°˜ë³µì •ë³´"):
                            repeat_info_markdown = gr.Markdown("ë°˜ë³µì •ë³´ íƒ­ì„ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.", elem_classes="tab-content-markdown")
                        with gr.TabItem("ì¶”ê°€ì´ë¯¸ì§€", id="ì¶”ê°€ì´ë¯¸ì§€"):
                            additional_images_gallery = gr.Gallery(label="ì¶”ê°€ ì´ë¯¸ì§€", columns=5, height="auto", object_fit="contain")
        
        # --- Event Handler & Parsing Functions ---

        async def update_sigungu_dropdown(province):
            if not province or province == "ì „êµ­": return gr.update(choices=[], value=None)
            try:
                sigungu_options = await scraper.get_sigungu_options(province)
                if "ì „ì²´" not in sigungu_options: sigungu_options.insert(0, "ì „ì²´")
                return gr.update(choices=sigungu_options, value="ì „ì²´")
            except Exception: return gr.update(choices=[], value=None)

        async def update_large_category_dropdown(tourism_type):
            if not tourism_type or tourism_type == "ì„ íƒ ì•ˆí•¨":
                return gr.update(choices=DEFAULT_LARGE_CATEGORIES, value="ì„ íƒ ì•ˆí•¨")
            
            options = await scraper.get_large_category_options(tourism_type)
            return gr.update(choices=["ì„ íƒ ì•ˆí•¨"] + options, value="ì„ íƒ ì•ˆí•¨")

        async def update_medium_category_dropdown(tourism_type, large_category):
            if not large_category or large_category == "ì„ íƒ ì•ˆí•¨": return gr.update(choices=[], value=None)
            options = await scraper.get_medium_category_options(tourism_type, large_category)
            return gr.update(choices=["ì„ íƒ ì•ˆí•¨"] + options, value="ì„ íƒ ì•ˆí•¨")

        async def update_small_category_dropdown(tourism_type, large_category, medium_category):
            if not medium_category or medium_category == "ì„ íƒ ì•ˆí•¨": return gr.update(choices=[], value=None)
            options = await scraper.get_small_category_options(tourism_type, large_category, medium_category)
            return gr.update(choices=["ì„ íƒ ì•ˆí•¨"] + options, value="ì„ íƒ ì•ˆí•¨")

        async def process_search(params, page_num, total_pages=0):
            page_num = int(page_num)
            
            # Yield a full list of 13 values to match the `search_outputs` length
            yield [
                f"{page_num} í˜ì´ì§€ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...", # status_output
                [], # results_output
                gr.update(), # api_accordion
                gr.update(), # request_url_output
                gr.update(), # response_xml_output
                gr.update(), # search_params
                gr.update(), # current_page
                gr.update(), # total_pages
                gr.update(), # page_number_input
                gr.update(), # total_pages_output
                gr.update(), # current_gallery_data
                gr.update(visible=False), # detail_view_column
                None, # csv_output_file
            ]

            try:
                search_args = params.copy()
                if search_args.get("sigungu") == "ì „ì²´": search_args["sigungu"] = None
                if search_args.get("tourism_type") == "ì„ íƒ ì•ˆí•¨": search_args["tourism_type"] = None
                if search_args.get("cat1") == "ì„ íƒ ì•ˆí•¨": search_args["cat1"] = None
                if search_args.get("cat2") == "ì„ íƒ ì•ˆí•¨": search_args["cat2"] = None
                if search_args.get("cat3") == "ì„ íƒ ì•ˆí•¨": search_args["cat3"] = None

                results, req_url, xml_res, total_count = await scraper.get_search_results(**search_args, pageNo=page_num, temp_dir=TEMP_DIR, totalPages=total_pages)
                
                if page_num == 1:
                    total_pages_val = math.ceil(total_count / ITEMS_PER_PAGE) if total_count > 0 else 1
                else:
                    total_pages_val = total_pages

                gallery_data = [(item['image'] if item.get('image') else NO_IMAGE_PLACEHOLDER_PATH, item['title']) for item in results]
                
                status_message = f"ì´ {total_count}ê°œ ê²€ìƒ‰ ì™„ë£Œ (í˜ì´ì§€ {page_num}/{total_pages_val})"
                if not results: 
                    status_message = "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

                # Yield a full list of 13 values
                yield [
                    status_message, # status_output
                    gallery_data, # results_output
                    gr.update(visible=True), # api_accordion
                    req_url, # request_url_output
                    xml_res, # response_xml_output
                    params, # search_params
                    page_num, # current_page
                    total_pages_val, # total_pages
                    page_num, # page_number_input
                    f"/ {total_pages_val}", # total_pages_output
                    results, # current_gallery_data
                    gr.update(visible=False), # detail_view_column
                    None # csv_output_file
                ]
            except Exception as e:
                # Yield a full list of 13 values on error
                yield [
                    f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", # status_output
                    [], # results_output
                    gr.update(), # api_accordion
                    gr.update(), # request_url_output
                    gr.update(), # response_xml_output
                    gr.update(), # search_params
                    gr.update(), # current_page
                    gr.update(), # total_pages
                    gr.update(), # page_number_input
                    gr.update(), # total_pages_output
                    gr.update(), # current_gallery_data
                    gr.update(visible=False), # detail_view_column
                    None, # csv_output_file
                ]

        async def initial_search(lang, prov, sig, tour, c1, c2, c3):
            params = {"language": lang, "province": prov, "sigungu": sig, "tourism_type": tour, "cat1": c1, "cat2": c2, "cat3": c3}
            async for update in process_search(params, 1, 0): yield update

        async def change_page(page_num, stored_params):
            async for update in process_search(stored_params, int(page_num)): yield update

        def parse_xml_to_html_table(xml_string):
            try:
                root = ET.fromstring(xml_string)
                item = root.find('.//body/items/item')
                if item is None: return "<p>ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.</p>"
                html = "<table>"
                for child in item:
                    tag_name = child.tag
                    if tag_name in ['contentid', 'contenttypeid', 'createdtime', 'modifiedtime', 'firstimage', 'firstimage2', 'cpyrhtDivCd', 'areacode', 'sigungucode', 'cat1', 'cat2', 'cat3', 'mapx', 'mapy', 'mlevel', 'overview', 'title']:
                        continue
                    tag_text = child.text.replace('\n', '<br>') if child.text else ''
                    html += f"<tr><td>{tag_name}</td><td>{tag_text}</td></tr>"
                html += "</table>"
                return html
            except Exception as e: return f"<p>XML íŒŒì‹± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}</p>"

        def parse_common_info_xml(xml_string):
            try:
                root = ET.fromstring(xml_string)
                item = root.find('.//body/items/item')
                if item is None: return {}
                return {child.tag: child.text for child in item if child.text is not None}
            except: return {}

        def parse_images_xml(xml_string):
            try:
                root = ET.fromstring(xml_string)
                urls = [item.find('originimgurl').text for item in root.findall('.//body/items/item') if item.find('originimgurl') is not None]
                local_paths = []
                for url in urls:
                    save_path = ""
                    try:
                        response = requests.get(url, stream=True)
                        response.raise_for_status()
                        filename = url.split('/')[-1].split('?')[0]
                        save_path = os.path.join(TEMP_DIR, filename)
                        with open(save_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                f.write(chunk)
                        local_paths.append(save_path)
                    except Exception:
                        pass
                return local_paths
            except Exception:
                return []

        async def show_initial_details(evt: gr.SelectData, s_params, g_data, c_page):
            if not g_data or evt.index is None:
                yield {detail_view_column: gr.update(visible=False)}
                return
            
            selected_item = g_data[evt.index]
            title = selected_item['title']
            
            info_for_tabs = s_params.copy()
            if info_for_tabs.get("province") == "ì „êµ­": info_for_tabs["province"] = None
            if info_for_tabs.get("tourism_type") == "ì„ íƒ ì•ˆí•¨": info_for_tabs["tourism_type"] = None
            if info_for_tabs.get("sigungu") == "ì „ì²´": info_for_tabs["sigungu"] = None
            if info_for_tabs.get("cat1") == "ì„ íƒ ì•ˆí•¨": info_for_tabs["cat1"] = None
            if info_for_tabs.get("cat2") == "ì„ íƒ ì•ˆí•¨": info_for_tabs["cat2"] = None
            if info_for_tabs.get("cat3") == "ì„ íƒ ì•ˆí•¨": info_for_tabs["cat3"] = None
            info_for_tabs.update({"contentid": selected_item.get("contentid"), "pageNo": c_page, "coords": {"mapx": selected_item.get("mapx"), "mapy": selected_item.get("mapy")}})

            yield {status_output: f"'{title}' ìƒì„¸ ì •ë³´ ë¡œë”© ì¤‘...", detail_view_column: gr.update(visible=False)}
            
            try:
                args = {k: v for k, v in info_for_tabs.items() if k not in ['coords']}
                args["tab_name"] = "ê³µí†µì •ë³´"
                
                xml_string = await scraper.get_item_detail_xml(args)
                
                if "<error>" in xml_string: raise ValueError(xml_string)
                
                common_data = parse_common_info_xml(xml_string)
                
                update_dict = {
                    status_output: f"'{title}' ìƒì„¸ ì •ë³´ ë¡œë“œ ì™„ë£Œ.",
                    detail_view_column: gr.update(visible=True),
                    detail_title: gr.update(value=f"### {common_data.get('title', '')}"),
                    detail_image: gr.update(value=common_data.get('firstimage')),
                    detail_overview: gr.update(value=common_data.get('overview')),
                    detail_info_table: gr.update(value=parse_xml_to_html_table(xml_string)),
                    selected_item_info: info_for_tabs,
                    map_group: gr.update(visible=False),
                    intro_info_markdown: "ì†Œê°œì •ë³´ íƒ­ì„ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    repeat_info_markdown: "ë°˜ë³µì •ë³´ íƒ­ì„ ì„ íƒí•˜ì—¬ ì •ë³´ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
                    additional_images_gallery: []
                }
                yield update_dict
            except Exception as e:
                yield {status_output: f"ìƒì„¸ ì •ë³´ ë¡œë”© ì¤‘ ì˜¤ë¥˜: {e}", detail_view_column: gr.update(visible=True), detail_title: "ì˜¤ë¥˜", detail_overview: str(e)}

        async def update_tab_content(evt: gr.SelectData, item_info):
            if not item_info or not evt:
                yield {intro_info_markdown: gr.update(), repeat_info_markdown: gr.update(), additional_images_gallery: gr.update()}
                return
            
            tab_name = evt.value
            yield {
                intro_info_markdown: "ë¡œë”© ì¤‘..." if tab_name == "ì†Œê°œì •ë³´" else gr.update(),
                repeat_info_markdown: "ë¡œë”© ì¤‘..." if tab_name == "ë°˜ë³µì •ë³´" else gr.update(),
                additional_images_gallery: [] if tab_name == "ì¶”ê°€ì´ë¯¸ì§€" else gr.update()
            }
            
            args = {k: v for k, v in item_info.items() if k not in ['coords']}
            args["tab_name"] = tab_name
            
            xml_string = await scraper.get_item_detail_xml(args)
            
            if "<error>" in xml_string:
                yield {
                    intro_info_markdown: xml_string if tab_name == "ì†Œê°œì •ë³´" else gr.update(),
                    repeat_info_markdown: xml_string if tab_name == "ë°˜ë³µì •ë³´" else gr.update(),
                    additional_images_gallery: []
                }
                return

            if tab_name == "ì†Œê°œì •ë³´": yield {intro_info_markdown: parse_xml_to_html_table(xml_string)}
            elif tab_name == "ë°˜ë³µì •ë³´": yield {repeat_info_markdown: parse_xml_to_html_table(xml_string)}
            elif tab_name == "ì¶”ê°€ì´ë¯¸ì§€": yield {additional_images_gallery: parse_images_xml(xml_string)}

        def show_map(item_info):
            coords = item_info.get('coords', {})
            mapx, mapy = coords.get('mapx'), coords.get('mapy')
            if not mapx or not mapy: return gr.update(value="<p>ì¢Œí‘œ ì •ë³´ê°€ ì—†ì–´ ì§€ë„ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.</p>")
            map_url = f"https://maps.google.com/maps?q={mapy},{mapx}&hl=ko&z=15&output=embed"
            return gr.update(value=f'<iframe src="{map_url}" style="width: 100%; height: 400px; border: none;"></iframe>')

        # --- Attach Event Handlers ---
        search_inputs = [language_dropdown, province_dropdown, sigungu_dropdown, tourism_type_dropdown, 
                         large_category_dropdown, medium_category_dropdown, small_category_dropdown]
        search_outputs = [status_output, results_output, api_accordion, request_url_output, response_xml_output, 
                          search_params, current_page, total_pages, page_number_input, total_pages_output, current_gallery_data, detail_view_column, csv_output_file]
        
        detail_outputs = [status_output, detail_view_column, detail_title, detail_image, detail_overview, 
                          detail_info_table, selected_item_info, map_group, 
                          intro_info_markdown, repeat_info_markdown, additional_images_gallery]

        province_dropdown.change(update_sigungu_dropdown, inputs=province_dropdown, outputs=sigungu_dropdown)
        tourism_type_dropdown.change(update_large_category_dropdown, inputs=tourism_type_dropdown, outputs=large_category_dropdown).then(lambda: (gr.update(choices=[], value=None), gr.update(choices=[], value=None)), outputs=[medium_category_dropdown, small_category_dropdown])
        large_category_dropdown.change(update_medium_category_dropdown, inputs=[tourism_type_dropdown, large_category_dropdown], outputs=medium_category_dropdown).then(lambda: gr.update(choices=[], value=None), outputs=[small_category_dropdown])
        medium_category_dropdown.change(update_small_category_dropdown, inputs=[tourism_type_dropdown, large_category_dropdown, medium_category_dropdown], outputs=small_category_dropdown)

        search_button.click(fn=initial_search, inputs=search_inputs, outputs=search_outputs, queue=True)
        export_csv_button.click(fn=export_details_to_csv, inputs=[search_params], outputs=[csv_output_file], queue=True)

        async def change_page(page_num, stored_params, total_pages=0):
            async for update in process_search(stored_params, int(page_num), total_pages): yield update

        async def go_to_first_page(p):
            async for update in change_page(1, p, 0): yield update
        async def go_to_prev_page(current_page_num, tp, p):
            async for update in change_page(max(1, int(current_page_num) - 1), p, tp): yield update
        async def go_to_next_page(current_page_num, tp, p):
            async for update in change_page(min(int(current_page_num) + 1, tp), p, tp): yield update
        async def go_to_last_page(tp, p):
            async for update in change_page(tp, p, tp): yield update
        async def go_to_specific_page(pn, tp, p):
            async for update in change_page(pn, p, tp): yield update

        first_page_button.click(fn=go_to_first_page, inputs=[search_params], outputs=search_outputs, queue=True)
        prev_page_button.click(fn=go_to_prev_page, inputs=[page_number_input, total_pages, search_params], outputs=search_outputs, queue=True)
        next_page_button.click(fn=go_to_next_page, inputs=[page_number_input, total_pages, search_params], outputs=search_outputs, queue=True)
        last_page_button.click(fn=go_to_last_page, inputs=[total_pages, search_params], outputs=search_outputs, queue=True)
        page_number_input.submit(fn=go_to_specific_page, inputs=[page_number_input, total_pages, search_params], outputs=search_outputs, queue=True)

        results_output.select(fn=show_initial_details, inputs=[search_params, current_gallery_data, current_page], outputs=detail_outputs, queue=True)

        detail_tabs.select(fn=update_tab_content, inputs=[selected_item_info], outputs=[intro_info_markdown, repeat_info_markdown, additional_images_gallery], queue=True)

        show_map_button.click(fn=show_map, inputs=[selected_item_info], outputs=[map_html]).then(lambda: gr.update(visible=True), outputs=[map_group])
        close_map_button.click(lambda: gr.update(visible=False), outputs=[map_group])

    return demo

def create_naver_search_tab():
    """'ë„¤ì´ë²„ ê²€ìƒ‰ (ì„ì‹œ)' íƒ­ì˜ UIë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    with gr.Blocks() as tab:
        gr.Markdown("### ë„¤ì´ë²„ ë¸”ë¡œê·¸ í›„ê¸° ê²€ìƒ‰ ë° ìš”ì•½")
        
        # --- ìƒíƒœ ë³€ìˆ˜ ---
        search_results_state = gr.State([])

        # --- UI ì»´í¬ë„ŒíŠ¸ ---
        with gr.Row():
            keyword_input = gr.Textbox(
                label="ê²€ìƒ‰í•  í–‰ì‚¬ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”",
                placeholder="ì˜ˆ: 2025 í•œê°• ë¶ˆë¹› ê³µì—°",
                lines=1,
                scale=3
            )
            search_button = gr.Button("ê²€ìƒ‰ ì‹¤í–‰", variant="primary", scale=1)
            summarize_button = gr.Button("ê²°ê³¼ ìš”ì•½í•˜ê¸°", scale=1)

        gr.Markdown("--- ")
        summary_output = gr.Markdown(label="ë°©ë¬¸ê° ê²½í—˜ ì¤‘ì‹¬ ìš”ì•½ (GPT-4.1-mini)")
        image_gallery = gr.Gallery(label="ë¸”ë¡œê·¸ ì´ë¯¸ì§€ ëª¨ì•„ë³´ê¸°", columns=6, height="auto")
        
        gr.Markdown("---")
        gr.Markdown("### ğŸ’¬ í›„ê¸° ê¸°ë°˜ ì±—ë´‡")
        gr.Markdown("ë¸”ë¡œê·¸ í›„ê¸° ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ë³´ì„¸ìš”. (ì˜ˆ: ì£¼ì°¨ ì •ë³´, ìœ ëª¨ì°¨ ëŒê¸° í¸í•œê°€ìš”?, ë¹„ ì˜¤ëŠ” ë‚  ê°€ë„ ê´œì°®ë‚˜ìš”?)")
        
        with gr.Row():
            question_input = gr.Textbox(label="ì§ˆë¬¸ ì…ë ¥", placeholder="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...", scale=4)
            ask_button = gr.Button("ì§ˆë¬¸í•˜ê¸°", scale=1)
        
        answer_output = gr.Markdown(label="ì±—ë´‡ ë‹µë³€")

        gr.Markdown("---")
        with gr.Row():
            raw_json_output = gr.Textbox(
                label="Raw JSON ê²°ê³¼", 
                lines=20, 
                interactive=False
            )
            formatted_output = gr.Markdown()
            
        # --- ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ---
        search_button.click(
            fn=search_naver_reviews_and_scrape,
            inputs=[keyword_input],
            outputs=[raw_json_output, formatted_output, search_results_state, image_gallery]
        )

        summarize_button.click(
            fn=summarize_blog_contents_stream,
            inputs=[search_results_state],
            outputs=[summary_output]
        )

        ask_button.click(
            fn=answer_question_from_reviews_stream,
            inputs=[question_input, search_results_state],
            outputs=[answer_output]
        )
    return tab

# --- Gradio TabbedInterfaceë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì²´ UI êµ¬ì„± ---
demo = gr.TabbedInterface(
    [create_location_search_tab(), create_area_search_tab(), create_seoul_search_ui(), create_naver_search_tab(), create_tour_api_playwright_tab()],
    tab_names=["ë‚´ ìœ„ì¹˜ë¡œ ê²€ìƒ‰", "ì§€ì—­/ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ (ê¸°ì¡´ TourAPI)", "ì„œìš¸ì‹œ ê´€ê´‘ì§€ ê²€ìƒ‰ (ì‹ ê·œ)", "ë„¤ì´ë²„ ê²€ìƒ‰ (ì„ì‹œ)", "Tour API ì§ì ‘ ì¡°íšŒ (Playwright)"],
    title="TourLens ê´€ê´‘ ì •ë³´ ì•±"
)

# --- ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ---
if __name__ == "__main__":
    # .env íŒŒì¼ ë° í•„ìˆ˜ í‚¤ í™•ì¸ (ê¸°ì¡´ TourAPI í‚¤ í™•ì¸ ë¶€ë¶„ì€ ì£¼ì„ ì²˜ë¦¬í•˜ê±°ë‚˜ ì‚­ì œ ê°€ëŠ¥)
    # if not os.getenv("TOUR_API_KTY"):
    #     print("TourAPI í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì— TOUR_API_KTYë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
    #     exit()
    if not os.getenv("NAVER_CLIENT_ID") or not os.getenv("NAVER_CLIENT_SECRET"):
        print("ë„¤ì´ë²„ ë¸”ë¡œê·¸ API ì¸ì¦ ì •ë³´ê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit()
    if not os.getenv("NAVER_TREND_CLIENT_ID") or not os.getenv("NAVER_TREND_CLIENT_SECRET"):
        print("ë„¤ì´ë²„ íŠ¸ë Œë“œ API ì¸ì¦ ì •ë³´ê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit()

    demo.launch(debug=True)